---
title: "A Handful of Pixels"
subtitle: "big science using small data"
author: "Koen Hufkens, PhD"

title-slide-attributes:
    data-background-color: "#0b2735"

format: 
  revealjs:
    slide-number: true
    logo: images/icon-512.png
    css: logo.css
    background-transition: fade
---


## Acknowledgements

```{r warning = FALSE, echo = FALSE}
library(tidyverse)
library(geodata)
library(MODISTools)
library(phenocamr)
library(terra)
library(GenSA)

phenology <- readRDS(here::here("data/phenology_2012.rds"))
dem <- terra::rast(here::here("data/srtm_38_03.tif"))
```

:::: {.columns align=center}

::: {.column width="50%"}
![](images/belspo_logo.png){height=150px}

![](images/nsf_logo.png){height=150px}

![](images/logo-full.png){width=75%}
:::

::: {.column width="50%"}
![](images/harvard_logo.png){height=150px}

![](images/ubern_logos.png){height=150px}

![](images/ugent_logo.png){height=150px}
:::

::::

## Rational

::: {.incremental}
- data as large as necessary
- ... but as small as possible
- lean computing
- first principles, before scaling
:::

::: {.notes}
This workshop leans on r packages written for the community by BlueGreen Labs, as well as a course designed to understand and manipulate geospatial data, as taught at the University of Bern.
:::

## Outline

::: {.incremental}
- Geospatial processing
  - reading data
  - finding data
- Phenology modelling
  - first principles
  - parameter optimization
:::

::: {.notes}
This workshop leans on R packages written for the community by BlueGreen Labs, as well as a course designed to understand and manipulate geospatial data, as taught at the University of Bern.
:::

## Loading all libraries

```{r}
library(tidyverse)
library(geodata)
library(MODISTools)
library(phenocamr)
library(terra)
library(GenSA)
```

## Geospatial processing in R

Key libraries

::: {.incremental}
- `{terra}` (replacing `{raster}`)
- `{sf}`
:::

::: {.notes}
Two main libraries underpin geospatial processing in R (for the most part). This is the terra library for raster data and the sf library for vector data.
:::

## Data types

```{r}
#| fig-cap: "Image by [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Raster_vector_tikz.svg)"
#| fig-align: "left"
#| out-width: "100%"
#| echo: FALSE
knitr::include_graphics("images/Raster_vector_tikz.svg")
```

::: {.notes}
Data types use discrete locations (pixels) or vectors to define regions. I will focus on raster data for the most part, as this is more common when scaling results.
:::

## Manipulating raster data

First we load the required `{terra}` library

```{r}
#| echo: TRUE
#| eval: FALSE

# comment
r <- rast("")
```

## Band math

First we load the required `{terra}` library

```{r}
#| echo: TRUE
#| eval: FALSE
#| code-line-numbers: "|2"

# comment
library(terra)
```

## Masking values

First we load the required `{terra}` library

```{r}
#| echo: TRUE
#| eval: FALSE
#| code-line-numbers: "|2"

# comment
library(terra)
```

## Finding geospatial data

::: {.incremental}
- How to find / access geospatial data?
  - Application Programme Interfaces (APIs)
  - bulk download
  - STAC
- Consider the source (data provenance)
:::

::: {.notes}
Finding and wrangling data is often the most time intensive part of an analysis. 80% + of your time will cover all aspects of data manipulation and exploration. You can use all the help you can get, and using these tools will speed things up and free time for creative work.
:::

## APIs

I will focus on two APIs `{MODISTools}`, `{ecmwfr}` and `{appeears}` which allow you to query point and gridded remote sensing or climate data.

::: {.incremental}
- `{MODISTools}` does not require a login, and will be used as an easy example.
- `{ecmwfr}`/`{appeears}` requires a login 
  - hard to demo within a shorter time frame
:::

## Common data sources

::: {.incremental}
- ECMWF Copernicus Data Services (CDS) `{ecmwfr}*`
- NASA EarthData and digital archives `{MODISTools}*` & `{appeears}*`
- National Ecosystem Observation Network (NEON) `{rnpn}`
- PEP725 `{phenor}*`
- ICOS `{icoscp}*` (partial support)
- GBIF `{rgbif}`
:::

## Common data sources

Scientific data repositories (open static data downloads or deposits)

::: {.incremental}
- Zenodo.org
- Dryad
- Figshare
:::

## Downloading DEM SRTM data

```{r}
#| echo: TRUE
#| eval: FALSE
#| code-line-numbers: "|1|3-7|9-14"
geodata::elevation_3s(
    lat = 46.6756,
    lon = 7.85480,
    path = tempdir()
  )

dem <- terra::rast(
  file.path(
    tempdir(),
    "srtm_38_03.tif"
    )
)
```

## Downloading DEM SRTM data

```{r}
#| echo: FALSE
#| eval: TRUE
plot(dem)
```

## Downloading MODIS data

```{r}
#| echo: TRUE
#| eval: FALSE
# list all products
MODISTools::mt_products()
```

## Downloading MODIS data
### listing products

```{r eval=FALSE}
library(MODISTools)
MODISTools::mt_products()
```
## Downloading MODIS data
### grab the data

```{r}
#| echo: TRUE
#| eval: FALSE

phenology <- MODISTools::mt_subset(
  product = "MCD12Q2",
  lat = 46.6756,
  lon = 7.85480,
  band = "Greenup.Num_Modes_01",
  start = "2012-01-01",
  end = "2012-12-31",
  km_lr = 100,
  km_ab = 100,
  site_name = "swiss",
  internal = TRUE,
  progress = FALSE
)
```

## Downloading MODIS data
### reformatting

Converting the dates, and the format

```{r}
#| echo: TRUE
#| eval: TRUE
#| code-line-numbers: "|2-7|9-12"
# screening of data
phenology <- phenology |>
  mutate(
    value = ifelse(value > 32656, NA, value),
    value = as.numeric(format(as.Date("1970-01-01") + value, "%j")),
    value = ifelse (value < 200, value, NA)
  )

# convert to raster format
phenology_raster <- MODISTools::mt_to_terra(
  phenology,
  reproject = TRUE
)

```

## Downloading MODIS data
### data vizualization

Visualizing the data!

```{r}
#| echo: TRUE
#| eval: TRUE

plot(phenology_raster)
```

## Resample

```{r}
#| echo: TRUE
#| eval: TRUE
#| code-line-numbers: "|1-5|7-11"

dem <- terra::resample(
  x = dem,
  y = phenology_raster,
  method = "average"
)

dem <- terra::mask(
  dem,
  is.na(phenology_raster),
  maskvalues = TRUE
)
```

## Combine data

```{r}
#| echo: TRUE
#| eval: TRUE
#| code-line-numbers: "|2-3|4-8"

# convert to data frame and merge
dem_df <- as.vector(dem)
phenology_df <- as.vector(phenology_raster)
sct_df <- data.frame(
  altitude = dem_df,
  doy = phenology_df
  )

```

## Fancy plot

:::: {.columns align=center}

::: {.column width="50%"}

```{r warning = FALSE, message= FALSE}
#| fig-align: "center"
#| out-width: "100%"
#| fig-width: 5
#| fig-height: 3
#| echo: TRUE
#| eval: FALSE

ggplot(
  data = sct_df,
  aes(altitude,doy)
  ) +
  geom_hex() +
  scale_fill_viridis_c(
    trans="log10"
    ) +
  geom_smooth(
    method = "lm",
    se = FALSE,
    colour = "white",
    lty = 2
  ) +
  labs(
    x = "altitude (m)",
    y = "greenup (DOY)"
  )
```

:::

::: {.column width="50%"}

```{r warning = FALSE, message= FALSE}
#| fig-align: "center"
#| out-width: "100%"
#| fig-width: 5
#| fig-height: 3
#| echo: FALSE
#| eval: TRUE

ggplot(data = sct_df, aes(altitude,doy)) +
  geom_hex() +
  scale_fill_viridis_c(trans="log10") +
  geom_smooth(
    method = "lm",
    se = FALSE,
    colour = "white",
    lty = 2
  ) +
  labs(
    x = "altitude (m)",
    y = "MODIS vegetation greenup (DOY)"
  )
```
:::

::::

## Summary stats {.smaller}

```{r}
#| echo: TRUE
#| eval: TRUE
fit <- lm(doy ~ altitude, data = sct_df)
print(summary(fit))
```

## Phenology modelling

Download a time series of LAI data

```{r}
#| echo: TRUE
#| eval: FALSE

df <- MODISTools::mt_subset(
  product = "MCD15A3H",
  lat = 42.536669726040884,
  lon = -72.17951595626516,
  band = "Lai_500m",
  start = "2002-01-01",
  end = "2022-12-31",
  km_lr = 0,
  km_ab = 0,
  site_name = "HF",
  internal = TRUE,
  progress = TRUE
)
```

## Phenology modelling

Convert dates

```{r}
#| echo: TRUE
#| eval: FALSE

df <- df |>
  mutate(
    # scale the values correctly
    value =  value * as.numeric(scale),
    date = as.Date(calendar_date),
    year = as.numeric(format(date, "%Y")) 
  )
```

## Phenology modelling

Convert dates

```{r}
#| echo: TRUE
#| eval: FALSE
#| code-line-numbers: "|1|3-11"
library(phenocamr)

phenocamr::download_phenocam(
  site = "harvard$",
  veg_type = "DB",
  roi_id = "1000",
  daymet = TRUE,
  phenophase = TRUE,
  trim = 2022,
  out_dir = tempdir()
  )

```

## Phenology modelling

```{r}
#| echo: TRUE
#| eval: FALSE

# time series
harvard_phenocam_data <- readr::read_csv(
  file.path(tempdir(), "harvard_DB_1000_3day.csv"), 
  comment = "#"
  )

# calculated phenology dates
harvard_phenology <- readr::read_csv(
  file.path(
    tempdir(),
    "harvard_DB_1000_3day_transition_dates.csv"
    ),
  comment = "#"
) |>
  dplyr::filter(
    direction == "rising",
    gcc_value == "gcc_90"
  )
```


## Phenology model

```{r}
#| echo: TRUE
#| eval: FALSE

gdd_model <- function(temp, par) {
  # split out parameters from a simple
  # vector of parameter values
  temp_threshold <- par[1]
  gdd_crit <- par[2]
  
  # accumulate growing degree days for
  # temperature data
  gdd <- cumsum(ifelse(temp > temp_threshold, temp - temp_threshold, 0))
  
  # figure out when the number of growing
  # degree days exceeds the minimum value
  # required for leaf development, only
  # return the first value
  doy <- unlist(which(gdd >= gdd_crit)[1])
  
  return(doy)
}

```

## Phenology model


```{r}
#| echo: TRUE
#| eval: FALSE

prediction <- harvard_temp |>
  dplyr::filter(
    year == 2010
  ) |>
  group_by(year) |>
  summarize(
    pred = gdd_model(
      temp = tmean,
      par = c(5, 130.44)
    )  
  )

print(prediction)

```

## Phenology model

```{r}
#| echo: TRUE
#| eval: FALSE

# run model and compare to true values
# returns the RMSE
rmse_gdd <- function(par, data) {
  
  # split out data
  drivers <- data$drivers
  validation <- data$validation
  
  # calculate phenology predictions
  # and put in a data frame
  predictions <- drivers |>
    group_by(year) |>
    summarise(
      predictions = gdd_model(
        temp = tmean,
        par = par
      )
    )
  
  predictions <- left_join(predictions, validation, by = "year")
  
  rmse <- predictions |>
    summarise(
      rmse = sqrt(mean((predictions - doy)^2, na.rm = TRUE))
    ) |>
    pull(rmse)
  
  # return rmse value
  return(rmse)
}

```

## Model optimization

```{r}
#| echo: TRUE
#| eval: FALSE

data <- list(
  drivers = harvard_temp,
  validation = harvard_phenology
  )

# optimize the model parameters
optim_par = GenSA::GenSA(
 par = c(0, 130),
 fn = rmse_gdd,
 lower = c(-10,0),
 upper = c(45,500),
 control = list(
   max.call = 4000
   ),
 data = data
)$par
```

## Model predictions

```{r}
#| echo: TRUE
#| eval: FALSE

# run the model for all years
# to get the phenology predictions
predictions <- harvard_temp |>
  group_by(year) |>
  summarize(
   prediction = gdd_model(
    temp = tmean,
    par = optim_par
  )  
  )
```

