---
title: "A Handful of Pixels"
subtitle: "big science using small data"
author: "Koen Hufkens, PhD"

title-slide-attributes:
    data-background-color: "#0b2735"

format: 
  revealjs:
    slide-number: true
    logo: images/logo_duo.png
    css: logo.css
    background-transition: fade
---

```{r warning = FALSE, echo = FALSE}
library(tidyverse)
library(geodata)
library(MODISTools)
library(phenocamr)
library(terra)
library(GenSA)
library(signal)

phenology <- readRDS(here::here("data/phenology_2012.rds"))
dem <- terra::rast(here::here("data/srtm_38_03.tif"))
ma_nh_temp <- terra::rast(here::here("data/daymet_mean_temperature.tif"))

harvard_phenology <- readr::read_csv(
    here::here("./data/harvard_DB_1000_3day_transition_dates.csv"),
  comment = "#"
) |>
  dplyr::filter(
    direction == "rising",
    gcc_value == "gcc_90"
  )


harvard_phenocam_data <- readr::read_csv(
  here::here("data/harvard_DB_1000_3day.csv"), 
  comment = "#"
  )

# return mean daily temperature as well
# as formal dates (for plotting)
harvard_temp <- harvard_phenocam_data |>
  dplyr::mutate(
    tmean = (tmax..deg.c. + tmin..deg.c.)/2,
    date = as.Date(date),
    year = as.numeric(format(date, "%Y"))
  )
```

## SLIDES

https://khufkens.github.io/FAIR_phenological_modelling/phenology_workshop.html

## {phenor}

```{r}
#| fig-align: "center"
#| out-width: "100%"
#| fig-cap: "https://bluegreen-labs.github.io/phenor/"
#| echo: FALSE
knitr::include_graphics("images/phenor.png")
```

## 

:::: {.columns align=center}

::: {.column width="50%"}

Drivers

- ERA5 / CMIP
- Berkley Earth
- DAYMET
- NASA NEX (CMIP downscaled)*
- E-OBS

:::

::: {.column width="50%"}

Targets

- US-NPN
- PhenoCam
- MODIS MCD12Q2
- PEP725
- custom formatting functions

:::

::::

## {.smaller}

:::: {.columns align=center}

:::{.column width="50%"}

Optimization supported by `{BayesianTools}`, optimizer zoo (with support for
priors through the control function) and cross validation through
`pr_cross_validate()`

```{r}
#| echo: TRUE
#| eval: FALSE

optim_par <- phenor::pr_fit(
  data = phenocam_DB,
  model = "TT",
  method = "BayesianTools",
  control = list(
    sampler = "DEzs",
    settings = list(
      burnin = 10,
      iterations = 10000)
    )
)
```
:::

:::{.column width="50%"}
```{r}
#| fig-align: "center"
#| out-width: "100%"
#| echo: FALSE
knitr::include_graphics("images/parameters.png")
```
:::
::::

## 

```{r}
#| fig-align: "center"
#| out-width: "100%"
#| echo: FALSE
knitr::include_graphics("images/sad.jpg")
```

## Rational

::: {.incremental}
- data as large as necessary
- ... but as small as possible
- lean computing
- first principles, before scaling
:::

::: {.notes}
This workshop leans on r packages written for the community by BlueGreen Labs, as well as a course designed to understand and manipulate geospatial data, as taught at the University of Bern.
:::

## Outline

::: {.incremental}
- Geospatial processing
  - reading data
  - finding data
- Phenology modelling
  - first principles
  - parameter optimization
  - scaling
:::

::: {.notes}
Note that these are toy examples, not best practices.
:::

## Loading all libraries

```{r}
#| echo: TRUE
#| eval: FALSE

# if you need to install all of them first use
# install.packages(
#   c("tidyverse",
#     "geodata",
#     "MODISTools",
#     "phenocamr",
#     "terra",
#     "GenSA"
#     )
#   )

# load all the libraries up front
library(tidyverse)
library(geodata)
library(MODISTools)
library(phenocamr)
library(terra)
library(GenSA)
```

## Geospatial processing in R

Key libraries

::: {.incremental}
- `{terra}` (replacing `{raster}`)
- `{sf}`
:::

::: {.notes}
Two main libraries underpin geospatial processing in R (for the most part). This is the terra library for raster data and the sf library for vector data.
:::

## Data types

```{r}
#| fig-cap: "Image by [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Raster_vector_tikz.svg)"
#| fig-align: "center"
#| out-width: "100%"
#| echo: FALSE
knitr::include_graphics("images/Raster_vector_tikz.svg")
```

::: {.notes}
Data types use discrete locations (pixels) or vectors to define regions. I will focus on raster data for the most part, as this is more common when scaling results.
:::

## Finding geospatial data

::: {.incremental}
- How to find / access geospatial data?
  - Application Programme Interfaces (APIs)
  - bulk download
  - STAC
- Consider the source (data provenance)
:::

::: {.notes}
Finding and wrangling data is often the most time intensive part of an analysis. 80% + of your time will cover all aspects of data manipulation and exploration. You can use all the help you can get, and using these tools will speed things up and free time for creative work.
:::

## APIs

I will focus on one APIs `{MODISTools}`. But, `{ecmwfr}` and `{appeears}` allow you to query point and gridded remote sensing or climate data as well.

::: {.incremental}
- `{MODISTools}` does not require a login, and will be used as an easy example.
- `{ecmwfr}`/`{appeears}` requires a login 
  - hard to demo within a shorter time frame
:::

## APIs

::: {.incremental}
- ECMWF Copernicus Data Services (CDS) `{ecmwfr}`
- NASA EarthData and digital archives `{MODISTools}` & `{appeears}`
:::

## Common data sources

Scientific data repositories (open static data downloads or deposits)

::: {.incremental}
- Zenodo.org
- Dryad
- Figshare
:::

## Downloading DEM SRTM data

```{r}
#| echo: TRUE
#| eval: FALSE
#| code-line-numbers: "|1-5|7-11|"
geodata::elevation_3s(
    lat = 46.6756,
    lon = 7.85480,
    path = tempdir()
  )

dem <- terra::rast(
  file.path(
    tempdir(),
    "srtm_38_03.tif"
    )
)
```

## 

```{r}
#| echo: FALSE
#| eval: TRUE
plot(dem)
```

## 

Listing data

```{r}
#| echo: TRUE
#| eval: FALSE
# list all products
MODISTools::mt_products()
```

## 

```{r}
#| eval: TRUE
MODISTools::mt_products()
```

## Downloading MODIS data

```{r}
#| echo: TRUE
#| eval: FALSE
#| code-line-numbers: "|2|3-4|5|6-7|8-9|10|11|"

phenology <- MODISTools::mt_subset(
  product = "MCD12Q2",
  lat = 46.6756,
  lon = 7.85480,
  band = "Greenup.Num_Modes_01",
  start = "2012-01-01",
  end = "2012-12-31",
  km_lr = 100,
  km_ab = 100,
  site_name = "swiss",
  internal = TRUE,
  progress = TRUE
)
```

## 

If downloads are too slow data can be downloaded from:
https://github.com/bluegreen-labs/handful_of_pixels/raw/main/data/phenology_2012.rds

```{r}
#| echo: TRUE
#| eval: FALSE

download.file(
  "https://github.com/bluegreen-labs/handful_of_pixels/raw/main/data/phenology_2012.rds",
  destfile = file.path(tempdir(),"phenology_2012.rds")
)

phenology <- readRDS(file.path(tempdir(),"phenology_2012.rds"))
```

## 

Converting the dates, and the format

```{r}
#| echo: TRUE
#| eval: TRUE
#| code-line-numbers: "|1-6|8-11|"
phenology <- phenology |>
  mutate(
    value = ifelse(value > 32656, NA, value),
    value = as.numeric(format(as.Date("1970-01-01") + value, "%j")),
    value = ifelse(value < 200, value, NA)
  )

# convert to raster format
phenology_raster <- MODISTools::mt_to_terra(
  phenology,
  reproject = TRUE
)

```

## 

Visualizing the data!

```{r}
#| echo: TRUE
#| eval: TRUE

plot(phenology_raster)
```

## 

Resampling DEM for comparison

```{r}
#| echo: TRUE
#| eval: TRUE
#| code-line-numbers: "|1-5|7-11|"

dem <- terra::resample(
  x = dem,
  y = phenology_raster,
  method = "average"
)

dem <- terra::mask(
  dem,
  is.na(phenology_raster),
  maskvalues = TRUE
)
```

## 

Combine data in one dataframe!

```{r}
#| echo: TRUE
#| eval: TRUE
#| code-line-numbers: "|2-3|" 
sct_df <- data.frame(
  altitude = as.vector(dem),
  doy = as.vector(phenology_raster)
  )
```

## 

:::: {.columns align=center}

::: {.column width="50%"}

```{r warning = FALSE, message= FALSE}
#| fig-align: "center"
#| out-width: "100%"
#| fig-width: 5
#| fig-height: 3
#| echo: TRUE
#| eval: FALSE
#| code-line-numbers: "|2|3|"

ggplot(
  data = sct_df,
  aes(altitude,doy)
  ) +
  geom_hex() +
  scale_fill_viridis_c(
    trans="log10"
    ) +
  geom_smooth(
    method = "lm",
    se = FALSE,
    colour = "white",
    lty = 2
  ) +
  labs(
    x = "altitude (m)",
    y = "greenup (DOY)"
  )
```

:::

::: {.column width="50%"}

```{r warning = FALSE, message= FALSE}
#| fig-align: "center"
#| out-width: "100%"
#| fig-width: 5
#| fig-height: 3
#| echo: FALSE
#| eval: TRUE

ggplot(data = sct_df, aes(altitude,doy)) +
  geom_hex() +
  scale_fill_viridis_c(trans="log10") +
  geom_smooth(
    method = "lm",
    se = FALSE,
    colour = "white",
    lty = 2
  ) +
  labs(
    x = "altitude (m)",
    y = "MODIS vegetation greenup (DOY)"
  )
```
:::

::::

## {.smaller}

```{r}
#| echo: TRUE
#| eval: TRUE
fit <- lm(doy ~ altitude, data = sct_df)
print(summary(fit))
```

Hopkins Law (four days / 400ft (120 m))

## Phenology modelling

Download time series, calculate phenophases

```{r}
#| echo: TRUE
#| eval: FALSE
#| code-line-numbers: "|2|3|4|5|6|7|8|"
phenocamr::download_phenocam(
  site = "harvard$",
  veg_type = "DB",
  roi_id = "1000",
  daymet = TRUE,
  phenophase = TRUE,
  trim = 2022,
  out_dir = tempdir()
  )

```

::: {.notes}
This routine downloads the phenocam data for the deciduous broadleaf class, calculates the phenophases, and aligns the data with matching daymet data. I'll use the daymet data in modelling. Although US centric the same routines apply for EU based modelling efforts. I fall back on DAYMET data for the ease of use.
:::

## 

Read the data

```{r}
#| echo: TRUE
#| eval: FALSE

harvard_phenocam_data <- readr::read_csv(
  file.path(tempdir(), "harvard_DB_1000_3day.csv"), 
  comment = "#"
  )

# return mean daily temperature as well
# as formal dates (for plotting)
harvard_temp <- harvard_phenocam_data |>
  dplyr::mutate(
    tmean = (tmax..deg.c. + tmin..deg.c.)/2,
    date = as.Date(date),
    year = as.numeric(format(date, "%Y"))
  )
```

## 

```{r}
#| echo: TRUE
#| eval: FALSE

# calculated phenology dates
harvard_phenology <- readr::read_csv(
  file.path(
    tempdir(),
    "harvard_DB_1000_3day_transition_dates.csv"
    ),
  comment = "#"
  ) |>
  dplyr::filter(
    direction == "rising",
    gcc_value == "gcc_90"
  )
```

::: {.notes}
This routine reads in the time series and calculated phenology dates, filtering out only the relevant spring characteristics.
:::

## 

Select phenophase and convert to year, doy

```{r}
#| echo: TRUE
#| eval: TRUE

harvard_phenology <- harvard_phenology |>
  mutate(
    doy = as.numeric(format(as.Date(transition_25),"%j")),
    year = as.numeric(format(as.Date(transition_25),"%Y"))
  ) |>
  select(
    year,
    doy,
    transition_25,
    threshold_25
    )
```

## 

Define the GDD model

```{r}
#| echo: TRUE
#| eval: TRUE
#| code-line-numbers: "|1|17|4-5|9|15|"

gdd_model <- function(temp, par) {
  # split out parameters from a simple
  # vector of parameter values
  temp_threshold <- par[1]
  gdd_crit <- par[2]
  
  # accumulate growing degree days for
  # temperature data
  gdd <- cumsum(ifelse(temp > temp_threshold, temp - temp_threshold, 0))
  
  # figure out when the number of growing
  # degree days exceeds the minimum value
  # required for leaf development, only
  # return the first value
  doy <- unlist(which(gdd >= gdd_crit)[1])
  
  return(doy)
}

```

::: {.notes}
Here I formulate a simple growing degree day model. With temperature as input, together with a list of parameters `par`, the threshold temperature and F* the critical growing degree days if exceeded should trigger phenology.
:::

## 

Run with fixed parameters

```{r}
#| echo: TRUE
#| eval: FALSE
#| code-line-numbers: "|2|3-8|"

prediction <- harvard_temp |>
  group_by(year) |>
  summarize(
    pred = gdd_model(
      temp = tmean,
      par = c(5, 130)
    )
  )
```

::: {.notes}
We can now run the model on our data using a simple call, with ad-hoc parameters, say a temperature threshold of 5 degrees C, and a GDD F* threshold of 130. However, these parameters are ad-hoc and not optimized for our current dataset. To do so we need to find the optimal parameters for this problem. This will require... parameter optimization.
:::

## 

```{r}
#| echo: FALSE
#| eval: TRUE
prediction <- harvard_temp |>
  group_by(year) |>
  summarize(
    pred = gdd_model(
      temp = tmean,
      par = c(5, 130)
    )
  )
```

```{r}
#| echo: TRUE
#| eval: TRUE
print(prediction)
```

## 

Defining a cost function

```{r}
#| echo: TRUE
#| eval: TRUE
#| code-line-numbers: "|3|6-7|9-16|"

# run model and compare to true values
# returns the RMSE
rmse_gdd <- function(par, data) {
  
  # split out data
  drivers <- data$drivers
  validation <- data$validation
  
  # calculate phenology predictions
  # and put in a data frame
  predictions <- drivers |>
    group_by(year) |>
    summarise(
      predictions = gdd_model(
        temp = tmean,
        par = par
      )
    )
  
  predictions <- left_join(predictions, validation, by = "year")
  
  rmse <- predictions |>
    summarise(
      rmse = sqrt(mean((predictions - doy)^2, na.rm = TRUE))
    ) |>
    pull(rmse)
  
  # return rmse value
  return(rmse)
}

```

## Simulated annealing

```{r}
#| fig-align: "center"
#| out-width: "100%"
#| echo: FALSE
knitr::include_graphics("images/SANN.gif")
```

## 

Combine the data and run the optimization

```{r}
#| echo: TRUE
#| eval: FALSE
#| code-line-numbers: "|1-4|7-15|8|9|10-11|12-14|"

data <- list(
  drivers = harvard_temp,
  validation = harvard_phenology
  )

# optimize the model parameters
optim_par = GenSA::GenSA(
 data = data,
 fn = rmse_gdd,
 lower = c(-10,0),
 upper = c(45,500),
 control = list(
   max.call = 4000
   )
)
```


```{r warning=FALSE, message=FALSE, include=FALSE}
#| echo: FALSE
#| eval: TRUE

data <- list(
  drivers = harvard_temp,
  validation = harvard_phenology
  )

# optimize the model parameters
optim_par = GenSA::GenSA(
 data = data,
 fn = rmse_gdd,
 lower = c(-10,0),
 upper = c(45,500),
 control = list(
   max.call = 400
   )
)
```

Print the optimal parameters

```{r}
#| echo: TRUE
#| eval: TRUE
print(optim_par$par)
```

## Model predictions

```{r}
#| echo: TRUE
#| eval: TRUE

# run the model for all years
# to get the phenology predictions
predictions <- harvard_temp |>
  group_by(year) |>
  summarize(
   prediction = gdd_model(
    temp = tmean,
    par = optim_par$par
  )  
  )
```

## 

```{r}
#| echo: TRUE
#| eval: TRUE

print(head(predictions))
```

## 

:::: {.columns align=center}

::: {.column width="50%"}

```{r}
#| echo: TRUE
#| eval: FALSE
validation <- left_join(
  predictions, 
  harvard_phenology
)

ggplot(validation) +
  geom_smooth(
    aes(doy, prediction),
    method = "lm"
  ) +
  geom_point(
    aes(doy, prediction)
  ) +
  geom_abline(
    intercept=0, slope=1
    ) +
  labs(
    x = "Observed (DOY)",
    y = "Predicted (DOY)"
  )
```

:::

::: {.column width="50%"}

```{r}
#| echo: FALSE
#| eval: TRUE

# join predicted with observed data
validation <- left_join(
  predictions,
  harvard_phenology
  )

ggplot(validation) +
  geom_smooth(
    aes(
      doy,
      prediction
    ),
    method = "lm"
  ) +
  geom_point(
    aes(doy,prediction
    )
  ) +
  geom_abline(
    intercept=0, slope=1
    ) +
  labs(
    x = "Observed (DOY)",
    y = "Predicted (DOY)"
  )
```

:::

::::

## Spatial scaling

Download spatial driver data (DAYMET)

:::: {.columns align=center}

::: {.column width="50%"}

```{r}
#| echo: TRUE
#| eval: FALSE

# Download daily data
daymetr::download_daymet_tiles(
  tiles = 11935,
  start = 2012,
  end = 2012,
  param = c("tmin","tmax"),
  path = tempdir(),
  silent = FALSE
  )

# calculate the daily mean values
r <- daymetr::daymet_grid_tmean(
  path = tempdir(),
  product = 11935,
  year = 2012,
  internal = TRUE
)

```
:::

::: {.column width="50%"}

```{r}
#| echo: TRUE
#| eval: FALSE
# reproject to lat lon
r <- terra::project(
  r,
  "+init=epsg:4326"
)

# subset to first 180 days (layers)
ma_nh_temp <- terra::subset(
  r,
  1:180
)
```
:::
::::

## 

Download data:

https://github.com/bluegreen-labs/handful_of_pixels/raw/main/data/daymet_mean_temperature.tif

`{terra}` allows loading remote data!!

```{r}
#| echo: TRUE
#| eval: FALSE
r <- rast("URL")
```

## Spatial scaling

Run the model on the spatial data, plot the results

```{r}
#| echo: TRUE
#| eval: TRUE

predicted_phenology <- terra::app(
  ma_nh_temp,
  fun = gdd_model,
  par = optim_par$par
)

```

## Spatial scaling


```{r}
#| echo: TRUE
#| eval: TRUE

plot(predicted_phenology)
```


## Full course

https://bluegreen-labs.github.io/handful_of_pixels/

